{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install openai\n",
    "# %pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def openaiCompletionArcs(parameters):\n",
    "    # load environment variables from .env file\n",
    "    load_dotenv('.secrets.env')\n",
    "    # set API key from environment variable\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    # call the OpenAI API to generate the list of story arcs\n",
    "    response = openai.Completion.create(\n",
    "        **parameters\n",
    "    )\n",
    "\n",
    "    # extract the list of story arcs from the API response\n",
    "    story_arcs = [arc.strip() for arc in response.choices[0].text.split(\"\\n\") if arc.strip()]\n",
    "\n",
    "    # pick a random story arc and store it as a variable\n",
    "    random_arc = random.choice(story_arcs)\n",
    "\n",
    "    return story_arcs, random_arc\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    \"model\": \"text-davinci-003\",\n",
    "    \"prompt\": \"List classic story arcs in fiction.\",\n",
    "    \"temperature\": 0, # 0 means no randomness 1 means completely random\n",
    "    \"max_tokens\": 50, # max number of tokens to generate\n",
    "    \"top_p\": 1, # 1 means no filtering, 0 means only the most likely token`\n",
    "    \"frequency_penalty\": 1, # 0 means no penalty, 1 means penalize new tokens based on their existing frequency\n",
    "    \"presence_penalty\": 1 # 0 means no penalty, 1 means penalize new tokens based on whether they appear in the prompt\n",
    "}\n",
    "\n",
    "# parameters = {\n",
    "#     \"model\": \"text-davinci-003\",\n",
    "#     \"prompt\": \"List classic story arcs in fiction.\",\n",
    "#     \"temperature\": 0,\n",
    "#     \"max_tokens\": 150,\n",
    "#     \"top_p\": 1,\n",
    "#     \"frequency_penalty\": 1,\n",
    "#     \"presence_penalty\": 1\n",
    "# }\n",
    "\n",
    "story_arcs, random_arc = openaiCompletionArcs(parameters)\n",
    "print(\"Story arcs:\")\n",
    "print(\"\\n\".join(story_arcs))\n",
    "print('\\n')\n",
    "print(\"Randomly selected arc:\")\n",
    "print(random_arc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Create a variable to store the chat history\n",
    "chat_history = []\n",
    "\n",
    "# Define a function to generate a response to a prompt\n",
    "\n",
    "def first_chat_request_response():\n",
    "    # load environment variables from .env file\n",
    "    load_dotenv('.secrets.env')\n",
    "    # set API key from environment variable\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    # Define the model name\n",
    "    MODEL = \"gpt-3.5-turbo\"\n",
    "    # Define the content to start the prompt\n",
    "    CONTENT = f\"You are a novelist writing in the {random_arc} arc style.\"\n",
    "    # Define the prompt\n",
    "    PROMPT = f\"Write the introduction to a novel in the {random_arc} arc style.\"\n",
    "    # Example OpenAI Python library request\n",
    "    response = openai.ChatCompletion.create(model=MODEL, messages=[{\"role\": \"system\", \"content\": CONTENT}, {\n",
    "                                            \"role\": \"user\", \"content\": PROMPT}], temperature=0.6)\n",
    "    # Get the message object from the first choice\n",
    "    message = response[\"choices\"][0][\"message\"]\n",
    "    # Format the message content as a dictionary\n",
    "    message_content = {\"role\": message[\"role\"], \"content\": message[\"content\"]}\n",
    "    # Append the message content to the chat history\n",
    "    chat_history.append(message_content)\n",
    "\n",
    "# Define a function to generate a response to a prompt\n",
    "def subsequent_chat_request_response():\n",
    "    # Example OpenAI Python library request\n",
    "    MODEL = \"gpt-3.5-turbo\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=MODEL,\n",
    "        messages=chat_history,\n",
    "        temperature=0.9,\n",
    "    )\n",
    "    # Get the message object from the first choice\n",
    "    message = response[\"choices\"][0][\"message\"]\n",
    "    # Format the message content as a dictionary\n",
    "    message_content = {\"role\": message[\"role\"], \"content\": message[\"content\"]}\n",
    "    # Append the message content to the chat history\n",
    "    chat_history.append(message_content)\n",
    "\n",
    "# Call the function to start the chat\n",
    "first_chat_request_response()\n",
    "\n",
    "# Call the function to generate a followup response\n",
    "subsequent_chat_request_response()\n",
    "\n",
    "# Call the function to generate a followup response\n",
    "subsequent_chat_request_response()\n",
    "\n",
    "print(random_arc)\n",
    "print(chat_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for i in range(len(chat_history)):\n",
    "# #     print(chat_history[i]['content'])\n",
    "\n",
    "# # Define the chat history as a string\n",
    "# chat_history_html = ''\n",
    "# for message in chat_history:\n",
    "#     chat_history_html += f'<p class=\"message\"><span class=\"role\">{message[\"role\"]}:</span> {message[\"content\"]}</p>\\n'\n",
    "\n",
    "# # Open index.html file in append mode\n",
    "# with open('index.html', 'a') as f:\n",
    "#     # Replace the comment with the chat history text using string formatting\n",
    "#     f.write(random_arc)\n",
    "#     f.write('<div id=\"chat-history\">\\n')\n",
    "#     f.write(chat_history_html)\n",
    "#     f.write('</div>\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_dotenv() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     openai\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mOPENAI_API_KEY\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[39m# load environment variables from .env file\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m load_env_vars()\n\u001b[1;32m     20\u001b[0m \u001b[39m# Load arcsDicts.json\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_json\u001b[39m():\n",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m, in \u001b[0;36mload_env_vars\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_env_vars\u001b[39m():\n\u001b[0;32m---> 13\u001b[0m     load_dotenv()\n\u001b[1;32m     14\u001b[0m     \u001b[39m# set API key from environment variable\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     openai\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mOPENAI_API_KEY\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m, in \u001b[0;36mload_dotenv\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_dotenv\u001b[39m():\n\u001b[1;32m      9\u001b[0m     \u001b[39m# load environment variables from .env file\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     load_dotenv(\u001b[39m'\u001b[39;49m\u001b[39m.secrets.env\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: load_dotenv() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import openai\n",
    "import os\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Define function to load environment variables from .env file\n",
    "def load_dotenv():\n",
    "    # load environment variables from .env file\n",
    "    load_dotenv('.secrets.env')\n",
    "\n",
    "def load_env_vars():\n",
    "    load_dotenv()\n",
    "    # set API key from environment variable\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# load environment variables from .env file\n",
    "load_env_vars()\n",
    "\n",
    "# Load arcsDicts.json\n",
    "\n",
    "def load_json():\n",
    "    with open('arcsDicts.json') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Call the function to get the selected arc and its phases\n",
    "data = load_json()\n",
    "random_arc_data = random.choice(data)\n",
    "random_arc = random_arc_data[\"arc\"]\n",
    "phases = list(random_arc_data[\"phases\"].values())\n",
    "\n",
    "print(\"Selected arc:\")\n",
    "print(random_arc)\n",
    "\n",
    "# Path: apiFunctions.py\n",
    "parameters = {\n",
    "    \"model\": \"text-davinci-003\",\n",
    "    \"prompt\": \"List classic story arcs in fiction.\",\n",
    "    \"temperature\": 0,  # 0 means no randomness 1 means completely random\n",
    "    \"max_tokens\": 50,  # max number of tokens to generate\n",
    "    \"top_p\": 1,  # 1 means no filtering, 0 means only the most likely token`\n",
    "    # 0 means no penalty, 1 means penalize new tokens based on their existing frequency\n",
    "    \"frequency_penalty\": 1,\n",
    "    # 0 means no penalty, 1 means penalize new tokens based on whether they appear in the prompt\n",
    "    \"presence_penalty\": 1\n",
    "}\n",
    "\n",
    "# Create a variable to store the chat history\n",
    "chat_history = []\n",
    "\n",
    "# Define a function to generate a response to a prompt\n",
    "def chat_request_response(prompt):\n",
    "    # Example OpenAI Python library request\n",
    "    MODEL = \"text-davinci-002\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=MODEL,\n",
    "        prompt=prompt,\n",
    "        temperature=0.9,\n",
    "        max_tokens=1000,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    # Get the message object from the first choice\n",
    "    message = response[\"choices\"][0][\"text\"].strip()\n",
    "    return message\n",
    "\n",
    "# Call the function to start the chat\n",
    "first_prompt = f\"Write the introduction to a novel in the {random_arc} arc style.\"\n",
    "response = chat_request_response(first_prompt)\n",
    "print(response)\n",
    "chat_history.append(response)\n",
    "\n",
    "# Call the function to generate subsequent responses\n",
    "for phase in phases:\n",
    "    prompt = f\"Write a paragraph in the '{phase}' phase of the {random_arc} arc style.\"\n",
    "    response = chat_request_response(prompt)\n",
    "    print(response)\n",
    "    chat_history.append(response)\n",
    "\n",
    "print(random_arc)\n",
    "print(chat_history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
